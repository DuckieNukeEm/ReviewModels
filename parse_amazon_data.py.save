import pandas as pd
import re
import gzip
import numpy as np
from multiprocessing import cpu_count #,parallel

from nltk import  pos_tag
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize, word_tokenize

from datetime import datetime

def parallelize(data, func, use_cores = None):
	"""appplies a function (func) across the a data frame (data), but parralizing
	it across the number of cores use_cores"""
	if use_cores is None:
		use_cores = cpu_count()

	data_split = np.array_split(data, use_cores)
	pool = Pool(use_cores)
	data = pd.concat(pool.map(func, data_split))
	pool.close()
	pool.join()
	return(data)

def parse(PATH: str):
	"""returns an itterable for each element in the raw gz.json data file"""
	g = gzip.open(PATH, 'r')
	for l in g:
		yield eval(l)

def getDF(PATH: str, SAVE_PATH: str, chunk_size: int = 100000, headers:list = [],fix_count: bool = False, fix_cat:bool = False):
	"""reads in chunks of the json file and flattens them out into a usable format"""
	# TODO Add ReviewID Field
	i = 0
	big_count = 0
	first_pass = True
	df = {}

	for d in parse(PATH):
		df[i] = d
		i += 1
		if i % chunk_size == 0:
			big_count += i
			print('Just finished record %i' % big_count)
			df_out = pd.DataFrame.from_dict(df,  orient = 'index',columns = headers )[headers]
			if fix_count:
				df_out[['good_count','bad_count']] = pd.DataFrame( df_out.helpful.astype(str).apply(lambda x: re.sub('\[|\]|\ ','',x)).str.split(',').values.tolist()) 
				df_out.drop(columns = ['helpful'], inplace = True)
#			if fix_cat:
#				True
#				df_out['categories'] = df.categories.apply(lambda x: re.sub('\[\[|\]\]','',x)
			if first_pass:
				df_out.to_csv(SAVE_PATH, header = True , sep = '|', index = False)
				first_pass = False
			else:
				with open(SAVE_PATH, 'a') as f:
					df_out.to_csv(f, header = False, sep = '|', index = False)
			i = 0
			df = {}

def simple_tagger(tag):
	""" takes a tag and simpleifies it down to four tags (or five) tags
	https://simonhessner.de/lemmatize-whole-sentences-with-python-and-nltks-wordnetlemmatizer/"""
	if tag[0] == 'J':
		return('a')
	elif tag[0] == 'V':
		return('v')
	elif tag[0] == 'N':
		return('n')
	elif tag[0] == 'R':
		return('r')
	else:
		return(None)

def split_parse_join(sent: str, lemma):
	"""takes a string, splits it, lemmatizies it, and then joins it back again"""
	nltk_tagged = pos_tag(word_tokenize(sent.lower()))
	simple_tagged = map(lambda x: (x[0], simple_tagger(x[1])), nltk_tagged)

	out_list = [word if tag is None else lemma.lemmatize(word,tag) for  word, tag in simple_tagged]
	return(" ".join(out_list))



def split_parse_join_PAR(sent: str):
	"""takes a string, splits it, lemmatizies it, and then joins it back again"""
	nltk_tagged = pos_tag(word_tokenize(sent.lower()))
	simple_tagged = map(lambda x: (x[0], simple_tagger(x[1])), nltk_tagged)

	lemma = WordNetLemmatizer()

	out_list = [word if tag is None else lemma.lemmatize(word,tag) for  word, tag in simple_tagged]
	return(" ".join(out_list))


def chunk_spj(load_path, save_path, chunksize = 10000):
	"""takes a pandas itterable and then itterates through it, apply it the spj from above
	and saving it down to the disk"""
	df_itter = pd.read_csv(load_path, sep = '|', chunksize = chunksize)
z	lemmatizer = WordNetLemmatizer()

	i = 0
	for chunk in df_itter:
		df_chunk = chunk.reviewText.copy()
		df_chunk = df_chunk.astype(str).apply(lambda x : split_parse_join(x, lemmatizer))
		df_chunk = df_chunk.apply(lambda x: re.sub('\|','',x))

		if i == 0:
			df_chunk.to_csv(save_path, index = False)
		else:
			with open(save_path, 'a') as f:
				df_chunk.to_csv(f, index = False, header = False)
		i += 1
		print('Working on Chunk %i' % i)
		if True: #used only for testing purposes
			break

def test():
	True

 def chunk_spj_PAR(load_path, save_path, chunksize = 10000):
	"""takes a pandas itterable and then itterates through it, apply it the spj from above and saving it down to the disk"""
	df_itter = pd.read_csv(load_path, sep = '|', chunksize = chunksize)

	def PAR_DEF(df):
		df = df.reviewText.copy()
		df = df.astype(str).apply(lambda x: split_parse_join_PAR(x))
		df = df.apply(lambda x: re.sub('\|', '', x))
	i = 0
	for chunk in df_itter:
		df_chunk = parallelize(chunk, PAR_DEF)

		if i == 0:
			df_chunk.to_csv(save_path, index = False)
		else:
			with open(save_path, 'a') as f:
				df_chunk.to_csv(f, index = False, header = False)
		i += 1
		print('Working on Chunk %i' % i)
		if True: #used only for testing purposes
			break


Start = datetime.now()

if False: #chagnge to True to unzip the user_dedup.json.gz or metadata.json.gz

	if True:
		PATH = '/home/beltain/R/Data/ARD/user_dedup.json.gz'
		SAVE_PATH = '/home/beltain/R/Data/ARD/amazon_review_data.txt'
		HEADERS = ['reviewerID', 'asin', 'reviewerName', 'unixReviewTime', 'reviewText','overall', 'reviewTime', 'summary','helpful']
		CHUNKSIZE = 5000000
		FIX_COUNT = True
		FIX_CAT = False
	else:
		PATH =      '/home/beltain/R/Data/ARD/metadata.json.gz'
		SAVE_PATH = '/home/beltain/R/Data/ARD/product_data.txt'
		HEADERS = ['asin','categories','salesRank','title','imUrl','price','description','brand','related']
		CHUNKSIZE = 9400000
		FIX_COUNT = False
		FIX_CAT = True

	getDF(PATH, SAVE_PATH, chunk_size = CHUNKSIZE, headers = HEADERS, fix_count = FIX_COUNT, fix_cat = FIX_CAT)

if True: # change to True if you want to lowercase, split, lemmatize, then join sentences

	PATH = '/home/beltain/R/Data/ARD/amazon_review_data.txt'
	SAVE_PATH = '/home/beltain/R/Data/ARD/ARD_clean_reviews.txt'
	CHUNKSIZE = 100000
	chunk_spj_PAR(PATH, SAVE_PATH, CHUNKSIZE)

End = datetime.now()

Delta = End - Start

print(Delta)
